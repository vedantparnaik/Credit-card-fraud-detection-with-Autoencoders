{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6492730,"sourceType":"datasetVersion","datasetId":3752264}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # !pip uninstall jax -y\n# # !pip install jax\n# incase you get error for tf wrt jax\n# we can use this to deploy","metadata":{"execution":{"iopub.status.busy":"2024-02-03T06:42:09.429743Z","iopub.execute_input":"2024-02-03T06:42:09.430013Z","iopub.status.idle":"2024-02-03T06:42:09.451069Z","shell.execute_reply.started":"2024-02-03T06:42:09.429992Z","shell.execute_reply":"2024-02-03T06:42:09.450513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install stable-baselines3[extra]\n# from stable_baselines3 import PPO\n# import gym","metadata":{"execution":{"iopub.status.busy":"2024-02-03T06:42:09.452141Z","iopub.execute_input":"2024-02-03T06:42:09.452625Z","iopub.status.idle":"2024-02-03T06:42:09.455244Z","shell.execute_reply.started":"2024-02-03T06:42:09.452605Z","shell.execute_reply":"2024-02-03T06:42:09.454712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastapi","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:42:54.989357Z","iopub.execute_input":"2024-02-03T17:42:54.989756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:44:03.881164Z","iopub.execute_input":"2024-02-03T17:44:03.881629Z","iopub.status.idle":"2024-02-03T17:44:04.534967Z","shell.execute_reply.started":"2024-02-03T17:44:03.881592Z","shell.execute_reply":"2024-02-03T17:44:04.533295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TODO**:\n* Lets save the model with hdf5 (h5)\n* Lets create a FAST API that will allow our model to interact with the outside world by receiving input data, processing it through the model, and returning predictions. FLASK is simple but we will be using FASTAPI as it can be easly deployed with AWS.\n* Integrate the ML model and API \n\n","metadata":{}},{"cell_type":"markdown","source":"**References**:\n\n* Dataset : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n* Reinforcement learning paper : https://arxiv.org/pdf/2112.04236.pdf \n* Ref for auto encoders : https://www.kaggle.com/code/robinteuwens/anomaly-detection-with-auto-encoders \n* FASTAPI : https://www.kaggle.com/code/abbascanguven/python-deploying-an-ml-model-with-fastapi","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom keras import regularizers\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix,recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/credit-card-fraud-detection-dataset-2023/creditcard_2023.csv')\nprint(\"shape of the data: \",data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headlist =[]\nprint(data.head())\nheadlist = data.columns.tolist()\nprint('data.shape', data.shape)\n# headlist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trues = (data['Class'] == 1).sum()\nprint(trues)\nfalses = (data['Class'] == 0).sum()\nprint(falses)\nunique_count = (data['Class'] == 0).unique()\nprint(unique_count)\n\n# Plotting\n# data.plot(kind='bar')  # You can change 'line' to 'bar', 'hist', 'box', etc.\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Test: shuffle the data and run the model. check the diff. might reduce bias from the pattern.","metadata":{}},{"cell_type":"code","source":"x = data.drop(['Class', 'id'], axis=1)\ny = data['Class']\n# print(x,y)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nx_train['Amount'] = scaler.fit_transform(x_train[['Amount']])\nx_test['Amount'] = scaler.transform(x_test[['Amount']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_dim = x_train.shape[1]\n\n# # Define the layers\n# input_layer = Input(shape=(input_dim,))\n# encoder = Dense(14, activation=\"tanh\")(input_layer)\n# encoder = Dense(7, activation=\"relu\")(encoder)\n# decoder = Dense(7, activation='tanh')(encoder)\n# decoder = Dense(input_dim, activation='relu')(decoder)\n\n# autoencoder = Model(inputs=input_layer, outputs=decoder)\n\n# # Training accuracy: 0.7691007333415402 \n# # Test accuracy: 0.7688655188787085","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = x_train.shape[1]\n\ninput_layer = Input(shape=(input_dim,))\n\nencoder = Dense(14, activation=\"tanh\")(input_layer)\nencoder = Dense(10, activation=\"relu\")(encoder)\nencoder = Dense(7, activation=\"relu\")(encoder)\n\ndecoder = Dense(7, activation='tanh')(encoder)\ndecoder = Dense(10, activation='relu')(decoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\n\nautoencoder = Model(inputs=input_layer, outputs=decoder)\n\n# Training accuracy: 0.784978808715685\n# Test accuracy: 0.7841126919086224","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_dim = x_train.shape[1]\n\n# input_layer = Input(shape=(input_dim,))\n\n# print(input_dim)\n# # print(input_layer)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_dim = x_train.shape[1]\n\n# input_layer = Input(shape=(input_dim,))\n\n# encoded = Dense(100, activation= 'tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n# encoded = Dense(50, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\n# encoded = Dense(25, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\n# encoded = Dense(12, activation = 'tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\n# encoded = Dense(6, activation='relu')(encoded)\n\n# # Decoder network\n# decoded = Dense(12, activation='tanh')(encoded)\n# decoded = Dense(25, activation='tanh')(decoded)\n# decoded = Dense(50, activation='tanh')(decoded)\n# decoded = Dense(100, activation='tanh')(decoded)\n\n# output_layer = Dense(x.shape[1], activation='relu')(decoded)\n\n# # Building a model\n# autoencoder = Model(input_layer, output_layer)\n\n# # Training accuracy: 0.8050182016425443\n# # Test accuracy: 0.8044510490125389\n# # do not remember number of epochs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_nonfraud = x_train[y_train == 0]\n\nautoencoder.compile(optimizer='adam', loss='mean_squared_error') # can be binary cross\nautoencoder.fit(x_train_nonfraud, x_train_nonfraud, epochs=1, batch_size=32, shuffle=True, validation_split=0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder.save('autoencoder_model.h5')\nautoencoder.save_weights('autoencoder_weights.h5')\n\n# autoencoder.load_weights('autoencoder_weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Calculate the Mean Squared Error (MSE)\ntrain_predictions = autoencoder.predict(x_train)\ntrain_mse = np.mean(np.power(x_train - train_predictions, 2), axis=1)\n\ntest_predictions = autoencoder.predict(x_test)\ntest_mse = np.mean(np.power(x_test - test_predictions, 2), axis=1)\n\n# 2. Determine a Threshold\nthreshold = np.percentile(train_mse[y_train == 0], 95)\n\n# 3. Evaluate Accuracy\ntrain_pred_labels = [1 if mse > threshold else 0 for mse in train_mse]\ntest_pred_labels = [1 if mse > threshold else 0 for mse in test_mse]\n\ntrain_accuracy = accuracy_score(y_train, train_pred_labels)\ntest_accuracy = accuracy_score(y_test, test_pred_labels)\n\nprint(\"Training accuracy:\", train_accuracy)\nprint(\"Test accuracy:\", test_accuracy)\n\n# confusion matrix and recall \n\n# confusion matrix for training data\ntrain_cm = confusion_matrix(y_train, train_pred_labels)\n\n# confusion matrix for testing data\ntest_cm = confusion_matrix(y_test, test_pred_labels)\n\nprint(\"Train confusionn matrix : \", train_cm)\nprint(\"Test confusion matrix : \", test_cm)\n#print(f\"Recall: {recall:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_predictions = autoencoder.predict(x_test)\nmse = np.mean(np.power(x_test - x_test_predictions, 2), axis=1)\nprint(max(mse))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport io\nfrom datetime import datetime\n\ndef setup_logging(timestamp):\n    \"\"\"Sets up the logging environment.\"\"\"\n    log_filename = f'logs/model_run_{timestamp}.log'\n    \n    # Ensure the logs directory exists\n    if not os.path.exists('logs'):\n        os.makedirs('logs')\n    \n    # Configure logging\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n    \n    # Check if logger has handlers already (important in Jupyter environments)\n    if not logger.handlers:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setLevel(logging.INFO)\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    \n    return logger\n\ndef log_model_details(timestamp, model_summary, epochs, training_accuracy, testing_accuracy):\n    \"\"\"\n    Logs the model details to the specified log file.\n    \n    :param timestamp: Timestamp for the log file\n    :param model_summary: Summary of the model architecture\n    :param epochs: Number of epochs used for training\n    :param training_accuracy: The training accuracy\n    :param testing_accuracy: The testing accuracy\n    \"\"\"\n    logger = setup_logging(timestamp)\n    \n    # Log the model summary\n    logger.info(\"Model Summary:\\n\" + model_summary)\n    \n    # Log the training details\n    logger.info(f\"Training for {epochs} epochs.\")\n    logger.info(f\"Training Accuracy: {training_accuracy}\")\n    logger.info(f\"Testing Accuracy: {testing_accuracy}\")\n    logger.info(\"***************************************\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stream = io.StringIO()\nautoencoder.summary(print_fn=lambda x: stream.write(x + '\\n'))\nsummary_string = stream.getvalue()\nstream.close()\n\n# Define your other parameters\ntimestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\nepochs = 10\ntraining_accuracy = 0.95  # Example value\ntesting_accuracy = 0.90  # Example value\n\n# Call the function to log everything\nlog_model_details(timestamp, summary_string, epochs, training_accuracy, testing_accuracy)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\n# Define a Policy-Based RL Environment\nclass FraudDetectionEnv(gym.Env):\n    def __init__(self):\n        super(FraudDetectionEnv, self).__init__()\n        self.action_space = gym.spaces.Discrete(n_actions)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_features + 1,))\n\n    def step(self, action):\n        # Implement the logic to handle actions and return the next state, reward, done, info\n        pass\n\n    def reset(self):\n        # Reset the environment to an initial state\n        pass\n\n# Initialize the environment and the RL agent\nenv = FraudDetectionEnv()\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\n\n# Train the RL agent\nmodel.learn(total_timesteps=10000)\n\n# Example of using the trained autoencoder and RL agent in tandem\ndef make_decision(transaction_data):\n    # Use autoencoder for anomaly detection\n    reconstructed = autoencoder.predict(transaction_data)\n    reconstruction_error = np.mean(np.abs(transaction_data - reconstructed), axis=1)\n\n    # Include reconstruction error in the state for the RL agent\n    state = np.concatenate([transaction_data, reconstruction_error[:, None]], axis=1)\n    action = model.predict(state, deterministic=True)[0]\n    \n    return action\n\n# Example transaction data (dummy data for illustration)\nsample_transaction = np.random.rand(1, n_features)\n\n# Make a decision on the transaction\ndecision = make_decision(sample_transaction)\nprint(\"Decision on transaction:\", decision)\n\"\"\"","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}